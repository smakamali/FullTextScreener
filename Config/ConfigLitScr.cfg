[general]
appVersion = 0.8.0
# Whether to index documents in `inputDir` before launching the chatbot UI
forceReindex = False
# Whether to log INFO and WARNING messages
enableLogging = True
# Whether to show low level progress bars, if set to False, only high level progress bar will be rendered. Only shown in the terminal.
showProgress = True


[llm-model]
# The service to use for LLM generation. Supported services: huggingface, mistral, llama3  
service = mistral  
# The LLM used by the chatbot components
# LLM generation parameters
# max number of tokens to generate in one round
max_new_tokens = 1100
# temperature for sampling from the LLM model (0.0 = greedy sampling, 1.0 = random sampling)
temperature = 0.0
 # whether to sample from the LLM model
do_sample = False

# [llama3]
# api_key = ###  
# model_name = meta-llama/Llama-3.2-1B 
# embed_model = intfloat/multilingual-e5-small 
# embedDim = 384 

[huggingface]
api_key = ###
model_name = mistralai/Mistral-7B-Instruct-v0.3
# model_name = Qwen/Qwen2.5-14B-Instruct-1M
# model_name = Qwen/Qwen2.5-1.5B-Instruct
# model_name = meta-llama/Llama-3.1-8B-Instruct
#model_name = Qwen/Qwen2.5-7B-Instruct
# model_name = meta-llama/Llama-3.1-8B-Instruct
#embed_model = BAAI/bge-small-en-v1.5
#model_name = google/gemma-3-12b-it
# model_name = google/gemma-2-9b-it
# model_name = google/gemma-3-1b-it
# embedDim = 384
# embed_model = Alibaba-NLP/gte-multilingual-base
embed_model = intfloat/multilingual-e5-small 

embedDim = 512


[mistral]
api_key = ###
model_name = mistral-large-latest
# embed_model = mistral-embed
# embedDim = 1024
embed_model = Alibaba-NLP/gte-multilingual-base
embedDim = 768

[neo4j]
username = neo4j
password = neo4j_rag_poc 
url1 = bolt://localhost:7687
url2 = http://localhost:7474
containerName = neo4j-apoc

[dir-structure]
inputDir = ./input/selected
input_pdf_folder = ./input/pdfs
metadataFile = ./input/metadata.csv
outputDir = ./output
outputFile = ./output/output.json
logDir = ./logs/
logFile = processing_log.txt
dataPath = ../neo4j_vol1/data
pluginsPath = ../neo4j_vol1/plugins
questions_file_path=./Config/Eva_questions.json


[agent]
# CHECK - TODO
# Whether to use stateless mode, if set to True the state of the chatbot is not stored in memory and the query engine is used directly. If set to False, a React agent is used to store the state of the chatbot.
enable_agent = False
# number of iterations to run the agent, if enable_agent is set to True
max_iterations = 10


[retriever]
# Default retriever parameters
vectorTopK = 10
cutoffScore = 0.5

[nodeparser]
# Node parser approach, can be either `static` or `semantic`
nodeParserType = static
# Number of documents to process at a time, larger values may lead to hitting the embedding API rate limit
batchSize = 1
# Number of tokens to include in each chunk
chunk_size = 1200
# The overlap between subsequent chunks
chunk_overlap = 200

# The following are applicable only to the Semantic Splitter
# Default = 1, Adjust buffer size as needed
bufferSize = 1
# Default = 95, lower values result in too many small chunks
breakpointPercentileThreshold = 95
